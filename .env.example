# AICLI Configuration Example
# Copy this file to .env and fill in your API keys

# OpenAI Configuration
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4

# Anthropic Configuration
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
ANTHROPIC_BASE_URL=https://api.anthropic.com
ANTHROPIC_MODEL=claude-3-sonnet-20240229

# Fireworks Configuration
FIREWORKS_API_KEY=your-fireworks-api-key-here
FIREWORKS_BASE_URL=https://api.fireworks.ai/inference/v1
FIREWORKS_MODEL=accounts/fireworks/models/llama-v2-70b-chat

# Together AI Configuration
TOGETHER_API_KEY=your-together-api-key-here
TOGETHER_BASE_URL=https://api.together.xyz/v1
TOGETHER_MODEL=meta-llama/Llama-2-70b-chat-hf

# Ollama Configuration (for local models)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# Default LLM Provider (openai, anthropic, fireworks, together, ollama)
DEFAULT_LLM_PROVIDER=anthropic

# CLI Configuration
DEBUG=false
LOG_LEVEL=INFO
LOG_FILE=aicli.log

# Session Management
SESSION_DIR=./sessions
MAX_SESSION_SIZE=10MB
AUTO_SAVE_SESSIONS=true

# Context Management
MAX_CONTEXT_SIZE=100000
CONTEXT_CACHE_SIZE=50MB
AUTO_LOAD_CONTEXT=true

# File Operations
BACKUP_FILES=true
BACKUP_DIR=./backups
MAX_FILE_SIZE=1MB

# Security Settings
ENABLE_SHELL_TOOLS=false
SHELL_WHITELIST=git,npm,pip,pytest
CONFIRM_DESTRUCTIVE_OPERATIONS=true

# Performance Settings
ASYNC_OPERATIONS=true
MAX_CONCURRENT_OPERATIONS=3
REQUEST_TIMEOUT=30

# Vector Search (optional)
ENABLE_VECTOR_SEARCH=false
VECTOR_DB_PATH=./embeddings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Rich Terminal Settings
RICH_THEME=aicli
ENABLE_SYNTAX_HIGHLIGHTING=true
ENABLE_MARKDOWN_RENDERING=true
TERMINAL_WIDTH=auto